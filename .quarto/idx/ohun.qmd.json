{"title":"<font size=\"7\"><b><i>ohun</i>&#x3A; optimizing acoustic signal detection</b></font>","markdown":{"yaml":{"title":"<font size=\"7\"><b><i>ohun</i>&#x3A; optimizing acoustic signal detection</b></font>"},"headingText":"**Objetive**","headingAttr":{"id":"","classes":["unnumbered","unlisted"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n::: {.alert .alert-info}\n\n-   Get familiar with concepts and data formatting practices related to automatic acoustic signals detection\n\n-   Learn how to run automatic detection using the package ohun\n:::\n\n \n\n[ohun](https://docs.ropensci.org/ohun/) is intended to facilitate the automated detection of sound events, providing functions to diagnose and optimize detection routines. Detections from other software can also be explored and optimized.\n\n \n\n::: {.alert .alert-warning}\n<font size = \"4\">The main features of the package are: </font>\n\n-   The use of reference annotations for detection optimization and diagnostic\n-   The use of signal detection theory indices to evaluate detection performance\n\n \n\n<font size = \"4\">The package offers functions for: </font>\n\n-   Curate references and acoustic data sets\n-   Diagnose detection performance\n-   Optimize detection routines based on reference annotations\n-   Energy-based detection\n-   Template-based detection\n:::\n\n \n\nAll functions allow the parallelization of tasks, which distributes the tasks among several processors to improve computational efficiency. The package works on sound files in '.wav', '.mp3', '.flac' and '.wac' format.\n\n------------------------------------------------------------------------\n\nTo install the latest developmental version from [github](https://github.com/) you will need the R package [remotes](https://cran.r-project.org/package=devtools):\n\n```{r, eval = FALSE}\n\n#load package\nlibrary(ohun)\nlibrary(tuneR)\nlibrary(seewave)\nlibrary(warbleR)\n\n```\n\n```{r global options, echo = FALSE, message=FALSE, warning=FALSE}\n\n#load package\nlibrary(ohun)\nlibrary(tuneR)\nlibrary(seewave)\nlibrary(warbleR)\n\n\ndata(lbh2)\ndata(lbh1)\ndata(lbh_reference)\n\n# for spectrograms\npar(mar = c(5, 4, 2, 2) + 0.1)\n\nstopifnot(require(knitr))\noptions(width = 90)\nopts_chunk$set(\n  comment = NA,\n  message = FALSE,\n  warning = FALSE,\n  # eval = if (isTRUE(exists(\"params\"))) params$EVAL else FALSE,\n  dev = \"jpeg\",\n  dpi = 100,\n  fig.asp = 0.4,\n  fig.width = 6.5,\n  out.width = \"100%\",\n  fig.align = \"center\"\n)\n\n```\n\n \n\n------------------------------------------------------------------------\n\n# Automatic sound event detection\n\nFinding the position of sound events in a sound file is a challenging task. [ohun](https://github.com/maRce10/ohun) offers two methods for automated sound event detection: template-based and energy-based detection. These methods are better suited for highly stereotyped or good signal-to-noise ratio (SNR) sounds, respectively. If the target sound events don't fit these requirements, more elaborated methods (i.e. machine learning approaches) are warranted:\n\n<figure>\n\n<center><img src=\"images/analysis_workflow.png\" alt=\"automated signal detection diagram\" width=\"500\" height=\"450\"/></center>\n\n<figcaption><i>Diagram depicting how target sound event features can be used to tell the most adequate sound event detection approach. Steps in which 'ohun' can be helpful are shown in color. (SNR = signal-to-noise ratio) </i></figcaption>\n\n</figure>\n\n \n\nStill, a detection run using other software can be optimized with the tools provided in [ohun](https://github.com/maRce10/ohun).\n\n \n\n# Signal detection theory applied to bioacoustics\n\nBroadly speaking, signal detection theory deals with the process of recovering signals (i.e. target signals) from background noise (not necessarily acoustic noise) and it's widely used for optimizing this decision making process in the presence of uncertainty. During a detection routine, the detected 'items' can be classified into 4 classes:\n\n-   **True positives (TPs)**: signals correctly identified as 'signal'\n-   **False positives (FPs)**: background noise incorrectly identified as 'signal'\n-   **False negatives (FNs)**: signals incorrectly identified as 'background noise'\n-   **True negatives (TNs)**: background noise correctly identified as 'background noise'\n\nSeveral additional indices derived from these indices are used to evaluate the performance of a detection routine. These are three useful indices in the context of sound event detection included in [ohun](https://github.com/maRce10/ohun):\n\n-   **Recall**: correct detections relative to total references (a.k.a. true positive rate or sensitivity; *TPs / (TPs + FNs)*)\n-   **Precision**: correct detections relative to total detections (*TPs / (TPs + FPs)*).\n-   **F1 score**: combines recall and precision as the harmonic mean of these two, so it provides a single value for evaluating performance (a.k.a. F-measure or Dice similarity coefficient).\n\n<font size = \"2\">*(Metrics that make use of 'true negatives' cannot be easily applied in the context of sound event detection as noise cannot always be partitioned in discrete units)*</font>\n\nA perfect detection will have no false positives or false negatives, which will result in both recall and precision equal to 1. However, perfect detection cannot always be reached and some compromise between detecting all target signals plus some noise (recall = 1 & precision \\< 1) and detecting only target signals but not all of them (recall \\< 1 & precision = 1) is warranted. The right balance between these two extremes will be given by the relative costs of missing signals and mistaking noise for signals. Hence, these indices provide an useful framework for diagnosing and optimizing the performance of a detection routine.\n\nThe package [ohun](https://github.com/maRce10/ohun) provides a set of tools to evaluate the performance of an sound event detection based on the indices described above. To accomplish this, the result of a detection routine is compared against a reference table containing the time position of all target sound events in the sound files. The package comes with an example reference table containing annotations of long-billed hermit hummingbird songs from two sound files (also supplied as example data: 'lbh1' and 'lbh2'), which can be used to illustrate detection performance evaluation. The example data can be explored as follows:\n\n```{r, eval = TRUE, }\n# load example data\ndata(\"lbh1\", \"lbh2\", \"lbh_reference\")\n\nlbh_reference\n```\n\n \n\n \n\nAll [ohun](https://github.com/maRce10/ohun) functions that work with this kind of data can take both selection tables and data frames. Spectrograms with highlighted sound events from a selection table can be plotted with the function `label_spectro()` (this function only plots one wave object at the time):\n\n```{r, eval = TRUE}\n# save sound file\nwriteWave(lbh1, file.path(tempdir(), \"lbh1.wav\"))\n\n# save sound file\nwriteWave(lbh2, file.path(tempdir(), \"lbh2.wav\"))\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  reference = lbh_reference[lbh_reference$sound.files == \"lbh1.wav\",],\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh2,\n  reference = lbh_reference[lbh_reference$sound.files == \"lbh2.wav\",],\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n```\n\nThe function `diagnose_detection()` evaluates the performance of a detection routine by comparing it to a reference table. For instance, a perfect detection is given by comparing `lbh_reference` to itself:\n\n```{r}\n\nlbh1_reference <-\n  lbh_reference[lbh_reference$sound.files == \"lbh1.wav\",]\n\n# diagnose\ndiagnose_detection(reference = lbh1_reference, \n                   detection = lbh1_reference)[, c(1:3, 7:9)]\n\n```\n\n \n\nWe will work mostly with a single sound file for convenience but the functions can work on several sound files at the time. The files should be found in a single working directory. Although the above example is a bit silly, it shows the basic diagnostic indices, which include basic detection theory indices ('true.positives', 'false.positives', 'false.negatives', 'recall' and 'precision') mentioned above. We can play around with the reference table to see how these indices can be used to spot imperfect detection routines (and hopefully improve them!). For instance, we can remove some sound events to see how this is reflected in the diagnostics. Getting rid of some rows in 'detection', simulating a detection with some false negatives, will affect the recall but not the precision:\n\n```{r}\n\n\n# create new table\nlbh1_detection <- lbh1_reference[3:9, ]\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  reference = lbh1_reference,\n  detection = lbh1_detection,\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# diagnose\ndiagnose_detection(reference = lbh1_reference, \n                   detection = lbh1_detection)[, c(1:3, 7:9)]\n\n```\n\n \n\nHaving some additional sound events not in reference will do the opposite, reducing precision but not recall. We can do this simply by switching the tables:\n\n```{r, }\n\n\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  detection = lbh1_reference,\n  reference = lbh1_detection,\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# diagnose\ndiagnose_detection(reference = lbh1_detection, \n                   detection = lbh1_reference)[, c(1:3, 7:9)]\n\n```\n\n \n\nThe function offers three additional diagnose metrics:\n\n-   **Split positives**: target sound events overlapped by more than 1 detecion\n-   **Merged positives**: number of cases in which 2 or more target sound events in 'reference' were overlapped by the same detection\n-   **Proportional overlap of true positives**: ratio of the time overlap of true positives with its corresponding sound event in the reference table\n\nIn a perfect detection routine split and merged positives should be 0 while proportional overlap should be 1. We can shift the start of sound events a bit to reflect a detection in which there is some mismatch to the reference table regarding to the time location of sound events:\n\n```{r, }\n\n# create new table\nlbh1_detection <- lbh1_reference\n\n# add 'noise' to start\nset.seed(18)\nlbh1_detection$start <-\n  lbh1_detection$start + rnorm(nrow(lbh1_detection), \n                               mean = 0, sd = 0.1)\n\n## print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  reference = lbh1_reference,\n  detection = lbh1_detection,\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# diagnose\ndiagnose_detection(reference = lbh1_reference, \n                   detection = lbh1_detection)\n\n```\n\n \n\nIn addition, the following diagnostics related to the duration of the sound events can also be returned by setting `time.diagnostics = TRUE`. Here we tweak the reference and detection data just to have some false positives and false negatives:\n\n```{r}\n\n# diagnose with time diagnostics\ndiagnose_detection(reference = lbh1_reference[-1, ], detection = lbh1_detection[-10, ], time.diagnostics = TRUE)\n\n```\n\n \n\nThese additional metrics can be used to further filter out undesired sound events based on their duration (for instance in a energy-based detection as in `energy_detector()`, explained below).\n\nDiagnostics can also be detailed by sound file:\n\n```{r}\n# diagnose by sound file\ndiagnostic <-\n  diagnose_detection(reference = lbh1_reference,\n                     detection = lbh1_detection,\n                     by.sound.file = TRUE)\n\ndiagnostic\n```\n\n \n\nThese diagnostics can be summarized (as in the default `diagnose_detection()` output) with the function `summarize_diagnostic()`:\n\n```{r}\n\n# summarize\nsummarize_diagnostic(diagnostic)\n\n```\n\n \n\n# Detecting sound events with *ohun*\n\n## Energy-based detection\n\nThis detector uses amplitude envelopes to infer the position of sound events. Amplitude envelopes are representations of the variation in energy through time. The following code plots an amplitude envelope along with the spectrogram for the example data `lbh1`:\n\n```{r, }\n\n# plot spectrogram and envelope\nlabel_spectro(\n  wave = cutw(\n    lbh1,\n    from = 0,\n    to = 1.5,\n    output = \"Wave\"\n  ),\n  ovlp = 90,\n  hop.size = 10,\n  flim = c(0, 10),\n  envelope = TRUE\n)\n\n```\n\n \n\nThis type of detector doesn't require highly stereotyped sound events, although they work better on high quality recordings in which the amplitude of target sound events is higher than the background noise (i.e. high signal-to-noise ratio). The function `energy_detector()` performs this type of detection.\n\n \n\n### How it works\n\nWe can understand how to use `energy_detector()` using simulated sound events. We will do that using the function `simulate_songs()` from [warbleR](https://CRAN.R-project.org/package=warbleR). In this example we simulate a recording with 10 sounds with two different frequency ranges and durations:\n\n```{r}\n\n# install this package first if not installed\n# install.packages(\"Sim.DiffProc\")\n\n#Creating vector for duration \ndurs <- rep(c(0.3, 1), 5)\n\n#Creating simulated song\nset.seed(12)\nsimulated_1 <-\n  warbleR::simulate_songs(\n    n = 10,\n    durs = durs,\n    freqs = 5,\n    sig2 = 0.01,\n    gaps = 0.5,\n    harms = 1,\n    bgn = 0.1,\n    path = tempdir(),\n    file.name = \"simulated_1\",\n    selec.table = TRUE,\n    shape = \"cos\",\n    fin = 0.3,\n    fout = 0.35,\n    samp.rate = 18\n  )$wave\n\n```\n\n \n\nThe function call saves a '.wav' sound file in a temporary directory (`tempdir()`) and also returns a `wave` object in the R environment. This outputs will be used to run energy-based detection and creating plots, respectively. This is how the spectrogram and amplitude envelope of the simulated recording look like:\n\n```{r, fig.height=4, fig.width=10}\n\n# plot spectrogram and envelope\nlabel_spectro(wave = simulated_1,\n              env = TRUE,\n              fastdisp = TRUE)\n\n```\n\n \n\nNote that the amplitude envelope shows a high signal-to-noise ratio of the sound events, which is ideal for energy-based detection. This can be conducted using `energy_detector()` as follows:\n\n```{r, fig.height=4, fig.width=10}\n\n\n# run detection\ndetection <-\nenergy_detector(\n  files = \"simulated_1.wav\",\n  bp = c(2, 8),\n  threshold = 50,\n  smooth = 150,\n  path = tempdir()\n)\n\n# plot spectrogram and envelope\nlabel_spectro(\nwave = simulated_1,\nenvelope = TRUE,\ndetection = detection,\nthreshold = 50\n)\n\n```\n\n \n\nThe output is a selection table:\n\n```{r}\n\ndetection\n\n```\n\nNow we will make use of some additional arguments to filter out specific sound events based on their structural features. For instance we can use the argument `min.duration` to provide a time treshold (in ms) to exclude short sound events and keep only the longest sound events:\n\n```{r eval = TRUE, echo = TRUE, fig.height=4, fig.width=10}\n\n\n# run detection\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(1, 8),\n    threshold = 50,\n    min.duration = 500,\n    smooth = 150,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1, detection = detection)\n\n```\n\n \n\nWe can use the argument `max.duration` (also in ms) to exclude long sound events and keep the short ones:\n\n```{r eval = TRUE, echo = TRUE, fig.height=4, fig.width=10}\n\n# run detection\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(1, 8),\n    threshold = 50,\n    smooth = 150,\n    max.duration = 500,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1,  detection = detection)\n\n```\n\n \n\nWe can also focus the detection on specific frequency ranges using the argument `bp` (bandpass). By setting `bp = c(5, 8)` only those sound events found within that frequency range (5-8 kHz) will be detected, which excludes sound events below 5 kHz:\n\n```{r, fig.height=4, fig.width=10, eval = TRUE, echo = TRUE}\n\n# Detecting\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(5, 8),\n    threshold = 50,\n    smooth = 150,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1,  detection = detection)\n\n```\n\n \n\n::: {.alert .alert-info}\n<font size=\"5\">Exercise</font>\n\n-   Detect only the short sound below 5 kHz\n\n```{r, fig.height=4, fig.width=10, eval = FALSE, echo = FALSE}\n\n# Detect\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(0, 6),\n    threshold = 50,\n    max.duration = 500,\n    smooth = 150,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1,  detection = detection)\n\n```\n:::\n\n \n\nAmplitude modulation (variation in amplitude across a sound event) can be problematic for detection based on amplitude envelopes. We can also simulate some amplitude modulation using `warbleR::simulate_songs()`:\n\n```{r, eval = TRUE, warning=FALSE, message=FALSE}\n\n#Creating simulated song\nset.seed(12)\n\n#Creating vector for duration\ndurs <- rep(c(0.3, 1), 5)\n\nsim_2 <-\n  simulate_songs(\n    n = 10,\n    durs = durs,\n    freqs = 5,\n    sig2 = 0.01,\n    gaps = 0.5,\n    harms = 1,\n    bgn = 0.1,\n    path = tempdir(),\n    file.name = \"simulated_2\",\n    selec.table = TRUE,\n    shape = \"cos\",\n    fin = 0.3,\n    fout = 0.35,\n    samp.rate = 18,\n    am.amps = c(1, 2, 3, 2, 0.1, 2, 3, 3, 2, 1)\n  )\n\n# extract wave object and selection table\nsimulated_2 <- sim_2$wave\nsim2_sel_table <- sim_2$selec.table\n\n# plot spectrogram\nlabel_spectro(wave = simulated_2, envelope = TRUE)\n\n```\n\n \n\nWhen sound events have strong amplitude modulation they can be split during detection:\n\n```{r, eval = TRUE}\n\n# detect sounds\ndetection <- energy_detector(files = \"simulated_2.wav\", threshold = 50, path = tempdir())\n\n# plot spectrogram\nlabel_spectro(wave = simulated_2, envelope = TRUE, threshold = 50, detection = detection)\n\n```\n\n \n\nThere are two arguments that can deal with this: `holdtime` and `smooth`. `hold.time` allows to merge split sound events that are found within a given time range (in ms). This time range should be high enough to merge things belonging to the same sound event but not too high so it merges different sound events. For this example a `hold.time` of 200 ms can do the trick (we know gaps between sound events are \\~0.5 s long):\n\n```{r, eval = TRUE}\n\n# detect sounds\ndetection <-\n  energy_detector(\n    files = \"simulated_2.wav\",\n    threshold = 50,\n    min.duration = 1,\n    path = tempdir(),\n    hold.time = 200\n  )\n\n# plot spectrogram\nlabel_spectro(\n  wave = simulated_2,\n  envelope = TRUE,\n  threshold = 50,\n  detection = detection\n)\n\n```\n\n \n\n`smooth` works by merging the amplitude envelope 'hills' of the split sound events themselves. It smooths envelopes by applying a sliding window averaging of amplitude values. It's given in ms of the window size. A `smooth` of 350 ms can merged back split sound events from our example:\n\n```{r, eval = TRUE}\n\n# detect sounds\ndetection <-\n  energy_detector(\n    files = \"simulated_2.wav\",\n    threshold = 50,\n    min.duration = 1,\n    path = tempdir(),\n    smooth = 350\n  )\n\n# plot spectrogram\nlabel_spectro(\n  wave = simulated_2,\n  envelope = TRUE,\n  threshold = 50,\n  detection = detection,\n  smooth = 350\n)\n\n```\n\n \n\nThe function has some additional arguments for further filtering detections (`peak.amplitude`) and speeding up analysis (`thinning` and `parallel`).\n\n \n\n### Optimizing energy-based detection\n\nThis last example using `smooth` can be used to showcase how the tunning parameters can be optimized. As explained above, to do this we need a reference table that contains the time position of the target sound events. The function `optimize_energy_detector()` can be used finding the optimal parameter values. We must provide the range of parameter values that will be evaluated:\n\n```{r}\n\noptim_detection <-\n  optimize_energy_detector(\n    reference = sim2_sel_table,\n    files = \"simulated_2.wav\",\n    threshold = 50,\n    min.duration = 1,\n    path = tempdir(),\n    smooth = c(100, 250, 350)\n  )\n\noptim_detection[, c(1, 2:5, 7:12, 17:18)]\n\n```\n\n \n\nThe output contains the combination of parameters used at each iteration as well as the corresponding diagnose indices. In this case all combinations generate a good detection (recall & precision = 1). However, only the routine with the highest `smooth` (last row) has no split sound events ('split.positive' column). It also shows a better overlap to the reference sound events ('overlap.to.true.positives' closer to 1).\n\nIn addition, there are two complementary functions for optimizing energy-based detection routines: `feature_reference()` and `merge_overlaps()`. `feature_reference()` allow user to get a sense of the time and frequency characteristics of a reference table. This information can be used to determine the range of tuning parameter values during optimization. This is the output of the function applied to `lbh_reference`:\n\n```{r}\n\nfeature_reference(reference = lbh_reference, path = tempdir())\n\n```\n\n \n\nFeatures related to selection duration can be used to set the 'max.duration' and 'min.duration' values, frequency related features can inform banpass values, gap related features inform hold time values and duty cycle can be used to evaluate performance. Peak amplitude can be used to keep only those sound events with the highest intensity, mostly useful for routines in which only a subset of the target sound events present in the recordings is needed.\n\n`merge_overlaps()` finds time-overlapping selections in reference tables and collapses them into a single selection. Overlapping selections would more likely appear as a single amplitude 'hill' and thus would be detected as a single sound event. So `merge_overlaps()` can be useful to prepare references in a format representing a more realistic expectation of how a pefect energy detection routine would look like.\n\n## Template-based detection\n\nThis detection method is better suited for highly stereotyped sound events. As it doesn't depend on the signal-to-noise ratio it's more robust to higher levels of background noise. The procedure is divided in three steps:\n\n-   Choosing the right template (`get_templates()`)\n-   Estimating the cross-correlation scores of templates along sound files (`template_correlator()`)\\\n-   Detecting sound events by applying a correlation threshold (`template_detector()`)\n\nThe function `get_templates()` can help you find a template closer to the average acoustic structure of the sound events in a reference table. This is done by finding the sound events closer to the centroid of the acoustic space. When the acoustic space is not supplied ('acoustic.space' argument) the function estimates it by measuring several acoustic parameters using the function [`spectro_analysis()`](https://marce10.github.io/warbleR/reference/spectro_analysis.html) from [`warbleR`](https://CRAN.R-project.org/package=warbleR)) and summarizing it with Principal Component Analysis (after z-transforming parameters). If only 1 template is required the function returns the sound event closest to the acoustic space centroid. The rationale here is that a sound event closest to the average sound event structure is more likely to share structural features with most sounds across the acoustic space than a sound event in the periphery of the space. These 'mean structure' templates can be obtained as follows:\n\n```{r, eval = FALSE, echo = TRUE}\n\n# get mean structure template\ntemplate <-\n  get_templates(reference = lbh1_reference, path = tempdir())\n\n```\n\n```{r, fig.width=6, fig.height=5, eval = TRUE, echo = FALSE}\n\npar(mar = c(5, 4, 1, 1), bg = \"white\")\n\n# get mean structure template\ntemplate <-\n  get_templates(reference = lbh1_reference, path = tempdir())\n\n```\n\n \n\nThe graph above shows the overall acoustic spaces, in which the sound closest to the space centroid is highlighted. The highlighted sound is selected as the template and can be used to detect similar sound events. The function `get_templates()` can also select several templates. This can be helpful when working with sounds that are just moderately stereotyped. This is done by dividing the acoustic space into sub-spaces defined as equal-size slices of a circle centered at the centroid of the acoustic space:\n\n```{r, eval = FALSE, echo = TRUE}\n\n# get 3 templates\nget_templates(reference = lbh_reference, \n                          n.sub.spaces = 3, path = tempdir())\n\n```\n\n```{r, fig.width=6, fig.height=5, eval = TRUE, echo = FALSE}\n\npar(mar = c(5, 4, 1, 1), bg = \"white\")\n\n# get 3 templates\ntemplates <- get_templates(reference = lbh_reference, \n                          n.sub.spaces = 3, path = tempdir())\n\n```\n\nWe will use the single template object ('template') to run a detection on the example 'lbh1' data:\n\n```{r}\n\n# get correlations\ncorrelations <-\n  template_correlator(templates = template,\n                      files = \"lbh1.wav\",\n                      path = tempdir())\n\n```\n\n \n\nThe output is an object of class 'template_correlations', with its own printing method:\n\n```{r}\n\n# print\ncorrelations\n\n```\n\n \n\nThis object can then be used to detect sound events using `template_detector()`:\n\n```{r}\n\n# run detection\ndetection <-\n  template_detector(template.correlations = correlations, threshold = 0.4)\n\ndetection\n```\n\n \n\nThe output can be explored by plotting the spectrogram along with the detection and correlation scores:\n\n```{r, warning=FALSE}\n\n# plot spectrogram\nlabel_spectro(\n  wave = lbh1,\n  detection = detection,\n  template.correlation = correlations[[1]],\n  flim = c(0, 10),\n  threshold = 0.4,\n  hop.size = 10, ovlp = 50)\n\n```\n\n \n\nThe performance can be evaluated using `diagnose_detection()`:\n\n```{r}\n\n#diagnose\ndiagnose_detection(reference = lbh1_reference, detection = detection)\n\n```\n\n \n\n### Optimizing template-based detection\n\nThe function `optimize_template_detector()` allows to evaluate the performance under different correlation thresholds:\n\n```{r}\n\n# run optimization\noptimization <-\n  optimize_template_detector(\n    template.correlations = correlations,\n    reference = lbh1_reference,\n    threshold = seq(0.1, 0.5, 0.1)\n  )\n\n# print output\noptimization\n```\n\n \n\nAdditional threshold values can be evaluated without having to run it all over again. We just need to supplied the output from the previous run with the argument `previous.output` (the same trick can be done when optimizing an energy-based detection):\n\n```{r}\n\n# run optimization\noptimize_template_detector(\n  template.correlations = correlations,\n  reference = lbh1_reference,\n  threshold = c(0.6, 0.7),\n  previous.output = optimization\n)\n\n```\n\n \n\nIn this case several threshold values can achieved an optimal detection.\n\n \n\n### Detecting several templates\n\nSeveral templates can be used within the same call. Here we correlate two templates on the two example sound files, taking one template from each sound file:\n\n```{r}\n\n# get correlations\ncorrelations <-\n  template_correlator(\n    templates = lbh_reference[c(1, 10),],\n    files = c(\"lbh1.wav\", \"lbh2.wav\"),\n    path = tempdir()\n  )\n\n# run detection\ndetection <-\n  template_detector(template.correlations = correlations, threshold = 0.5)\n\ncorrelations <-\n  template_correlator(\n    templates = lbh_reference[c(1, 10),],\n    files = c(\"lbh1.wav\", \"lbh2.wav\"),\n    path = tempdir()\n  )\n\n```\n\n \n\nNote that in these cases we can get the same sound event detected several times (duplicates), one by each template. We can check if that is the case just by diagnosing the detection:\n\n```{r}\n\n#diagnose\ndiagnose_detection(reference = lbh_reference, detection = detection)\n\n```\n\n \n\nDuplicates are shown as split positives. Fortunately, we can leave a single detected sound event by leaving only those with the highest correlation. To do this we first need to label each row in the detection using `label_detection()` and then remove duplicates using `filter_detection()`:\n\n```{r}\n\n# labeling detection\nlabeled <-\n  label_detection(reference = lbh_reference, detection = detection)\n\n```\n\nThis function adds a column ('detection.class') with the class label for each row:\n\n```{r}\n\ntable(labeled$detection.class)\n\n```\n\n \n\nNow we can filter out duplicates and diagnose the detection again, telling the function to select a single row per duplicate using the correlation score as a criterium (`by = \"scores\"`, this column is part of the `template_detector()` output):\n\n```{r}\n\n# filter\nfiltered <- filter_detection(detection = labeled, by = \"scores\")\n\n# diagnose\ndiagnose_detection(reference = lbh_reference, detection = filtered)\n```\n\n \n\nWe successfully get rid of duplicates and detected every single target sound event.\n\n------------------------------------------------------------------------\n\n## Improving detection speed\n\nDetection routines can take a long time when working with large amounts of acoustic data (e.g. large sound files and/or many sound files). These are some useful points to keep in mine when trying to make a routine more time-efficient:\n\n-   Always test procedures on small data subsets\n-   `template_detector()` is faster than `energy_detector()`\n-   Parallelization (see `parallel` argument in most functions) can significantly speed-up routines, but works better on Unix-based operating systems (linux and mac OS)\n-   Sampling rate matters: detecting sound events on low sampling rate files goes faster, so we should avoid having nyquist frequencies (sampling rate / 2) way higher than the highest frequency of the target sound events (sound files can be downsampled using warbleR's [`fix_sound_files()`](https://marce10.github.io/warbleR/reference/selection_table.html))\n-   Large sound files can make the routine crash, use `split_acoustic_data()` to split both reference tables and files into shorter clips.\n-   Think about using a computer with lots of RAM memory or a computer cluster for working on large amounts of data\n-   `thinning` argument (which reduces the size of the amplitude envelope) can also speed-up `energy_detector()`\n\n \n\n------------------------------------------------------------------------\n\n## Additional tips\n\n-   Use your knowledge about the sound event structure to determine the initial range for the tuning parameters in a detection optimization routine\n-   If people have a hard time figuring out where a target sound event occurs in a recording, detection algorithms will also have a hard time\n-   Several templates representing the range of variation in sound event structure can be used to detect semi-stereotyped sound events\n-   Make sure reference tables contain all target sound events and only the target sound events. The performance of the detection cannot be better than the reference itself.\n-   Avoid having overlapping sound events or several sound events as a single one (like a multi-syllable vocalization) in the reference table when running an energy-based detector\n-   Low-precision can be improved by training a classification model (e.g. random forest) to tell sound events from noise\n\n------------------------------------------------------------------------\n\n::: {.alert .alert-info}\nPlease cite [ohun](https://github.com/maRce10/ohun) like this:\n\nAraya-Salas, M. (2021), *ohun: diagnosing and optimizing automated sound event detection*. R package version 0.1.0.\n:::\n\n \n\n## References\n\n1.  Araya-Salas, M. (2021), ohun: diagnosing and optimizing automated sound event detection. R package version 0.1.0.\n2.  Araya-Salas M, Smith-Vidaurre G (2017) warbleR: An R package to streamline analysis of animal sound events. Methods Ecol Evol 8:184-191.\n3.  Khanna H., Gaunt S.L.L. & McCallum D.A. (1997). Digital spectrographic cross-correlation: tests of sensitivity. Bioacoustics 7(3): 209-234.\n4.  Knight, E.C., Hannah, K.C., Foley, G.J., Scott, C.D., Brigham, R.M. & Bayne, E. (2017). Recommendations for acoustic recognizer performance assessment with application to five common automated signal recognition programs. Avian Conservation and Ecology,\n5.  Macmillan, N. A., & Creelman, C.D. (2004). Detection theory: A user's guide. Psychology press.\n\n \n\n------------------------------------------------------------------------\n\n<font size=\"4\">Session information</font>\n\n```{r session info, echo=F}\n\nsessionInfo()\n\n```\n","srcMarkdownNoYaml":"\n\n::: {.alert .alert-info}\n## **Objetive** {.unnumbered .unlisted}\n\n-   Get familiar with concepts and data formatting practices related to automatic acoustic signals detection\n\n-   Learn how to run automatic detection using the package ohun\n:::\n\n \n\n[ohun](https://docs.ropensci.org/ohun/) is intended to facilitate the automated detection of sound events, providing functions to diagnose and optimize detection routines. Detections from other software can also be explored and optimized.\n\n \n\n::: {.alert .alert-warning}\n<font size = \"4\">The main features of the package are: </font>\n\n-   The use of reference annotations for detection optimization and diagnostic\n-   The use of signal detection theory indices to evaluate detection performance\n\n \n\n<font size = \"4\">The package offers functions for: </font>\n\n-   Curate references and acoustic data sets\n-   Diagnose detection performance\n-   Optimize detection routines based on reference annotations\n-   Energy-based detection\n-   Template-based detection\n:::\n\n \n\nAll functions allow the parallelization of tasks, which distributes the tasks among several processors to improve computational efficiency. The package works on sound files in '.wav', '.mp3', '.flac' and '.wac' format.\n\n------------------------------------------------------------------------\n\nTo install the latest developmental version from [github](https://github.com/) you will need the R package [remotes](https://cran.r-project.org/package=devtools):\n\n```{r, eval = FALSE}\n\n#load package\nlibrary(ohun)\nlibrary(tuneR)\nlibrary(seewave)\nlibrary(warbleR)\n\n```\n\n```{r global options, echo = FALSE, message=FALSE, warning=FALSE}\n\n#load package\nlibrary(ohun)\nlibrary(tuneR)\nlibrary(seewave)\nlibrary(warbleR)\n\n\ndata(lbh2)\ndata(lbh1)\ndata(lbh_reference)\n\n# for spectrograms\npar(mar = c(5, 4, 2, 2) + 0.1)\n\nstopifnot(require(knitr))\noptions(width = 90)\nopts_chunk$set(\n  comment = NA,\n  message = FALSE,\n  warning = FALSE,\n  # eval = if (isTRUE(exists(\"params\"))) params$EVAL else FALSE,\n  dev = \"jpeg\",\n  dpi = 100,\n  fig.asp = 0.4,\n  fig.width = 6.5,\n  out.width = \"100%\",\n  fig.align = \"center\"\n)\n\n```\n\n \n\n------------------------------------------------------------------------\n\n# Automatic sound event detection\n\nFinding the position of sound events in a sound file is a challenging task. [ohun](https://github.com/maRce10/ohun) offers two methods for automated sound event detection: template-based and energy-based detection. These methods are better suited for highly stereotyped or good signal-to-noise ratio (SNR) sounds, respectively. If the target sound events don't fit these requirements, more elaborated methods (i.e. machine learning approaches) are warranted:\n\n<figure>\n\n<center><img src=\"images/analysis_workflow.png\" alt=\"automated signal detection diagram\" width=\"500\" height=\"450\"/></center>\n\n<figcaption><i>Diagram depicting how target sound event features can be used to tell the most adequate sound event detection approach. Steps in which 'ohun' can be helpful are shown in color. (SNR = signal-to-noise ratio) </i></figcaption>\n\n</figure>\n\n \n\nStill, a detection run using other software can be optimized with the tools provided in [ohun](https://github.com/maRce10/ohun).\n\n \n\n# Signal detection theory applied to bioacoustics\n\nBroadly speaking, signal detection theory deals with the process of recovering signals (i.e. target signals) from background noise (not necessarily acoustic noise) and it's widely used for optimizing this decision making process in the presence of uncertainty. During a detection routine, the detected 'items' can be classified into 4 classes:\n\n-   **True positives (TPs)**: signals correctly identified as 'signal'\n-   **False positives (FPs)**: background noise incorrectly identified as 'signal'\n-   **False negatives (FNs)**: signals incorrectly identified as 'background noise'\n-   **True negatives (TNs)**: background noise correctly identified as 'background noise'\n\nSeveral additional indices derived from these indices are used to evaluate the performance of a detection routine. These are three useful indices in the context of sound event detection included in [ohun](https://github.com/maRce10/ohun):\n\n-   **Recall**: correct detections relative to total references (a.k.a. true positive rate or sensitivity; *TPs / (TPs + FNs)*)\n-   **Precision**: correct detections relative to total detections (*TPs / (TPs + FPs)*).\n-   **F1 score**: combines recall and precision as the harmonic mean of these two, so it provides a single value for evaluating performance (a.k.a. F-measure or Dice similarity coefficient).\n\n<font size = \"2\">*(Metrics that make use of 'true negatives' cannot be easily applied in the context of sound event detection as noise cannot always be partitioned in discrete units)*</font>\n\nA perfect detection will have no false positives or false negatives, which will result in both recall and precision equal to 1. However, perfect detection cannot always be reached and some compromise between detecting all target signals plus some noise (recall = 1 & precision \\< 1) and detecting only target signals but not all of them (recall \\< 1 & precision = 1) is warranted. The right balance between these two extremes will be given by the relative costs of missing signals and mistaking noise for signals. Hence, these indices provide an useful framework for diagnosing and optimizing the performance of a detection routine.\n\nThe package [ohun](https://github.com/maRce10/ohun) provides a set of tools to evaluate the performance of an sound event detection based on the indices described above. To accomplish this, the result of a detection routine is compared against a reference table containing the time position of all target sound events in the sound files. The package comes with an example reference table containing annotations of long-billed hermit hummingbird songs from two sound files (also supplied as example data: 'lbh1' and 'lbh2'), which can be used to illustrate detection performance evaluation. The example data can be explored as follows:\n\n```{r, eval = TRUE, }\n# load example data\ndata(\"lbh1\", \"lbh2\", \"lbh_reference\")\n\nlbh_reference\n```\n\n \n\n \n\nAll [ohun](https://github.com/maRce10/ohun) functions that work with this kind of data can take both selection tables and data frames. Spectrograms with highlighted sound events from a selection table can be plotted with the function `label_spectro()` (this function only plots one wave object at the time):\n\n```{r, eval = TRUE}\n# save sound file\nwriteWave(lbh1, file.path(tempdir(), \"lbh1.wav\"))\n\n# save sound file\nwriteWave(lbh2, file.path(tempdir(), \"lbh2.wav\"))\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  reference = lbh_reference[lbh_reference$sound.files == \"lbh1.wav\",],\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh2,\n  reference = lbh_reference[lbh_reference$sound.files == \"lbh2.wav\",],\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n```\n\nThe function `diagnose_detection()` evaluates the performance of a detection routine by comparing it to a reference table. For instance, a perfect detection is given by comparing `lbh_reference` to itself:\n\n```{r}\n\nlbh1_reference <-\n  lbh_reference[lbh_reference$sound.files == \"lbh1.wav\",]\n\n# diagnose\ndiagnose_detection(reference = lbh1_reference, \n                   detection = lbh1_reference)[, c(1:3, 7:9)]\n\n```\n\n \n\nWe will work mostly with a single sound file for convenience but the functions can work on several sound files at the time. The files should be found in a single working directory. Although the above example is a bit silly, it shows the basic diagnostic indices, which include basic detection theory indices ('true.positives', 'false.positives', 'false.negatives', 'recall' and 'precision') mentioned above. We can play around with the reference table to see how these indices can be used to spot imperfect detection routines (and hopefully improve them!). For instance, we can remove some sound events to see how this is reflected in the diagnostics. Getting rid of some rows in 'detection', simulating a detection with some false negatives, will affect the recall but not the precision:\n\n```{r}\n\n\n# create new table\nlbh1_detection <- lbh1_reference[3:9, ]\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  reference = lbh1_reference,\n  detection = lbh1_detection,\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# diagnose\ndiagnose_detection(reference = lbh1_reference, \n                   detection = lbh1_detection)[, c(1:3, 7:9)]\n\n```\n\n \n\nHaving some additional sound events not in reference will do the opposite, reducing precision but not recall. We can do this simply by switching the tables:\n\n```{r, }\n\n\n\n# print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  detection = lbh1_reference,\n  reference = lbh1_detection,\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# diagnose\ndiagnose_detection(reference = lbh1_detection, \n                   detection = lbh1_reference)[, c(1:3, 7:9)]\n\n```\n\n \n\nThe function offers three additional diagnose metrics:\n\n-   **Split positives**: target sound events overlapped by more than 1 detecion\n-   **Merged positives**: number of cases in which 2 or more target sound events in 'reference' were overlapped by the same detection\n-   **Proportional overlap of true positives**: ratio of the time overlap of true positives with its corresponding sound event in the reference table\n\nIn a perfect detection routine split and merged positives should be 0 while proportional overlap should be 1. We can shift the start of sound events a bit to reflect a detection in which there is some mismatch to the reference table regarding to the time location of sound events:\n\n```{r, }\n\n# create new table\nlbh1_detection <- lbh1_reference\n\n# add 'noise' to start\nset.seed(18)\nlbh1_detection$start <-\n  lbh1_detection$start + rnorm(nrow(lbh1_detection), \n                               mean = 0, sd = 0.1)\n\n## print spectrogram\nlabel_spectro(\n  wave = lbh1,\n  reference = lbh1_reference,\n  detection = lbh1_detection,\n  hop.size = 10,\n  ovlp = 50,\n  flim = c(1, 10)\n)\n\n# diagnose\ndiagnose_detection(reference = lbh1_reference, \n                   detection = lbh1_detection)\n\n```\n\n \n\nIn addition, the following diagnostics related to the duration of the sound events can also be returned by setting `time.diagnostics = TRUE`. Here we tweak the reference and detection data just to have some false positives and false negatives:\n\n```{r}\n\n# diagnose with time diagnostics\ndiagnose_detection(reference = lbh1_reference[-1, ], detection = lbh1_detection[-10, ], time.diagnostics = TRUE)\n\n```\n\n \n\nThese additional metrics can be used to further filter out undesired sound events based on their duration (for instance in a energy-based detection as in `energy_detector()`, explained below).\n\nDiagnostics can also be detailed by sound file:\n\n```{r}\n# diagnose by sound file\ndiagnostic <-\n  diagnose_detection(reference = lbh1_reference,\n                     detection = lbh1_detection,\n                     by.sound.file = TRUE)\n\ndiagnostic\n```\n\n \n\nThese diagnostics can be summarized (as in the default `diagnose_detection()` output) with the function `summarize_diagnostic()`:\n\n```{r}\n\n# summarize\nsummarize_diagnostic(diagnostic)\n\n```\n\n \n\n# Detecting sound events with *ohun*\n\n## Energy-based detection\n\nThis detector uses amplitude envelopes to infer the position of sound events. Amplitude envelopes are representations of the variation in energy through time. The following code plots an amplitude envelope along with the spectrogram for the example data `lbh1`:\n\n```{r, }\n\n# plot spectrogram and envelope\nlabel_spectro(\n  wave = cutw(\n    lbh1,\n    from = 0,\n    to = 1.5,\n    output = \"Wave\"\n  ),\n  ovlp = 90,\n  hop.size = 10,\n  flim = c(0, 10),\n  envelope = TRUE\n)\n\n```\n\n \n\nThis type of detector doesn't require highly stereotyped sound events, although they work better on high quality recordings in which the amplitude of target sound events is higher than the background noise (i.e. high signal-to-noise ratio). The function `energy_detector()` performs this type of detection.\n\n \n\n### How it works\n\nWe can understand how to use `energy_detector()` using simulated sound events. We will do that using the function `simulate_songs()` from [warbleR](https://CRAN.R-project.org/package=warbleR). In this example we simulate a recording with 10 sounds with two different frequency ranges and durations:\n\n```{r}\n\n# install this package first if not installed\n# install.packages(\"Sim.DiffProc\")\n\n#Creating vector for duration \ndurs <- rep(c(0.3, 1), 5)\n\n#Creating simulated song\nset.seed(12)\nsimulated_1 <-\n  warbleR::simulate_songs(\n    n = 10,\n    durs = durs,\n    freqs = 5,\n    sig2 = 0.01,\n    gaps = 0.5,\n    harms = 1,\n    bgn = 0.1,\n    path = tempdir(),\n    file.name = \"simulated_1\",\n    selec.table = TRUE,\n    shape = \"cos\",\n    fin = 0.3,\n    fout = 0.35,\n    samp.rate = 18\n  )$wave\n\n```\n\n \n\nThe function call saves a '.wav' sound file in a temporary directory (`tempdir()`) and also returns a `wave` object in the R environment. This outputs will be used to run energy-based detection and creating plots, respectively. This is how the spectrogram and amplitude envelope of the simulated recording look like:\n\n```{r, fig.height=4, fig.width=10}\n\n# plot spectrogram and envelope\nlabel_spectro(wave = simulated_1,\n              env = TRUE,\n              fastdisp = TRUE)\n\n```\n\n \n\nNote that the amplitude envelope shows a high signal-to-noise ratio of the sound events, which is ideal for energy-based detection. This can be conducted using `energy_detector()` as follows:\n\n```{r, fig.height=4, fig.width=10}\n\n\n# run detection\ndetection <-\nenergy_detector(\n  files = \"simulated_1.wav\",\n  bp = c(2, 8),\n  threshold = 50,\n  smooth = 150,\n  path = tempdir()\n)\n\n# plot spectrogram and envelope\nlabel_spectro(\nwave = simulated_1,\nenvelope = TRUE,\ndetection = detection,\nthreshold = 50\n)\n\n```\n\n \n\nThe output is a selection table:\n\n```{r}\n\ndetection\n\n```\n\nNow we will make use of some additional arguments to filter out specific sound events based on their structural features. For instance we can use the argument `min.duration` to provide a time treshold (in ms) to exclude short sound events and keep only the longest sound events:\n\n```{r eval = TRUE, echo = TRUE, fig.height=4, fig.width=10}\n\n\n# run detection\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(1, 8),\n    threshold = 50,\n    min.duration = 500,\n    smooth = 150,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1, detection = detection)\n\n```\n\n \n\nWe can use the argument `max.duration` (also in ms) to exclude long sound events and keep the short ones:\n\n```{r eval = TRUE, echo = TRUE, fig.height=4, fig.width=10}\n\n# run detection\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(1, 8),\n    threshold = 50,\n    smooth = 150,\n    max.duration = 500,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1,  detection = detection)\n\n```\n\n \n\nWe can also focus the detection on specific frequency ranges using the argument `bp` (bandpass). By setting `bp = c(5, 8)` only those sound events found within that frequency range (5-8 kHz) will be detected, which excludes sound events below 5 kHz:\n\n```{r, fig.height=4, fig.width=10, eval = TRUE, echo = TRUE}\n\n# Detecting\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(5, 8),\n    threshold = 50,\n    smooth = 150,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1,  detection = detection)\n\n```\n\n \n\n::: {.alert .alert-info}\n<font size=\"5\">Exercise</font>\n\n-   Detect only the short sound below 5 kHz\n\n```{r, fig.height=4, fig.width=10, eval = FALSE, echo = FALSE}\n\n# Detect\ndetection <-\n  energy_detector(\n    files = \"simulated_1.wav\",\n    bp = c(0, 6),\n    threshold = 50,\n    max.duration = 500,\n    smooth = 150,\n    path = tempdir()\n  )\n\n# plot spectrogram\nlabel_spectro(wave = simulated_1,  detection = detection)\n\n```\n:::\n\n \n\nAmplitude modulation (variation in amplitude across a sound event) can be problematic for detection based on amplitude envelopes. We can also simulate some amplitude modulation using `warbleR::simulate_songs()`:\n\n```{r, eval = TRUE, warning=FALSE, message=FALSE}\n\n#Creating simulated song\nset.seed(12)\n\n#Creating vector for duration\ndurs <- rep(c(0.3, 1), 5)\n\nsim_2 <-\n  simulate_songs(\n    n = 10,\n    durs = durs,\n    freqs = 5,\n    sig2 = 0.01,\n    gaps = 0.5,\n    harms = 1,\n    bgn = 0.1,\n    path = tempdir(),\n    file.name = \"simulated_2\",\n    selec.table = TRUE,\n    shape = \"cos\",\n    fin = 0.3,\n    fout = 0.35,\n    samp.rate = 18,\n    am.amps = c(1, 2, 3, 2, 0.1, 2, 3, 3, 2, 1)\n  )\n\n# extract wave object and selection table\nsimulated_2 <- sim_2$wave\nsim2_sel_table <- sim_2$selec.table\n\n# plot spectrogram\nlabel_spectro(wave = simulated_2, envelope = TRUE)\n\n```\n\n \n\nWhen sound events have strong amplitude modulation they can be split during detection:\n\n```{r, eval = TRUE}\n\n# detect sounds\ndetection <- energy_detector(files = \"simulated_2.wav\", threshold = 50, path = tempdir())\n\n# plot spectrogram\nlabel_spectro(wave = simulated_2, envelope = TRUE, threshold = 50, detection = detection)\n\n```\n\n \n\nThere are two arguments that can deal with this: `holdtime` and `smooth`. `hold.time` allows to merge split sound events that are found within a given time range (in ms). This time range should be high enough to merge things belonging to the same sound event but not too high so it merges different sound events. For this example a `hold.time` of 200 ms can do the trick (we know gaps between sound events are \\~0.5 s long):\n\n```{r, eval = TRUE}\n\n# detect sounds\ndetection <-\n  energy_detector(\n    files = \"simulated_2.wav\",\n    threshold = 50,\n    min.duration = 1,\n    path = tempdir(),\n    hold.time = 200\n  )\n\n# plot spectrogram\nlabel_spectro(\n  wave = simulated_2,\n  envelope = TRUE,\n  threshold = 50,\n  detection = detection\n)\n\n```\n\n \n\n`smooth` works by merging the amplitude envelope 'hills' of the split sound events themselves. It smooths envelopes by applying a sliding window averaging of amplitude values. It's given in ms of the window size. A `smooth` of 350 ms can merged back split sound events from our example:\n\n```{r, eval = TRUE}\n\n# detect sounds\ndetection <-\n  energy_detector(\n    files = \"simulated_2.wav\",\n    threshold = 50,\n    min.duration = 1,\n    path = tempdir(),\n    smooth = 350\n  )\n\n# plot spectrogram\nlabel_spectro(\n  wave = simulated_2,\n  envelope = TRUE,\n  threshold = 50,\n  detection = detection,\n  smooth = 350\n)\n\n```\n\n \n\nThe function has some additional arguments for further filtering detections (`peak.amplitude`) and speeding up analysis (`thinning` and `parallel`).\n\n \n\n### Optimizing energy-based detection\n\nThis last example using `smooth` can be used to showcase how the tunning parameters can be optimized. As explained above, to do this we need a reference table that contains the time position of the target sound events. The function `optimize_energy_detector()` can be used finding the optimal parameter values. We must provide the range of parameter values that will be evaluated:\n\n```{r}\n\noptim_detection <-\n  optimize_energy_detector(\n    reference = sim2_sel_table,\n    files = \"simulated_2.wav\",\n    threshold = 50,\n    min.duration = 1,\n    path = tempdir(),\n    smooth = c(100, 250, 350)\n  )\n\noptim_detection[, c(1, 2:5, 7:12, 17:18)]\n\n```\n\n \n\nThe output contains the combination of parameters used at each iteration as well as the corresponding diagnose indices. In this case all combinations generate a good detection (recall & precision = 1). However, only the routine with the highest `smooth` (last row) has no split sound events ('split.positive' column). It also shows a better overlap to the reference sound events ('overlap.to.true.positives' closer to 1).\n\nIn addition, there are two complementary functions for optimizing energy-based detection routines: `feature_reference()` and `merge_overlaps()`. `feature_reference()` allow user to get a sense of the time and frequency characteristics of a reference table. This information can be used to determine the range of tuning parameter values during optimization. This is the output of the function applied to `lbh_reference`:\n\n```{r}\n\nfeature_reference(reference = lbh_reference, path = tempdir())\n\n```\n\n \n\nFeatures related to selection duration can be used to set the 'max.duration' and 'min.duration' values, frequency related features can inform banpass values, gap related features inform hold time values and duty cycle can be used to evaluate performance. Peak amplitude can be used to keep only those sound events with the highest intensity, mostly useful for routines in which only a subset of the target sound events present in the recordings is needed.\n\n`merge_overlaps()` finds time-overlapping selections in reference tables and collapses them into a single selection. Overlapping selections would more likely appear as a single amplitude 'hill' and thus would be detected as a single sound event. So `merge_overlaps()` can be useful to prepare references in a format representing a more realistic expectation of how a pefect energy detection routine would look like.\n\n## Template-based detection\n\nThis detection method is better suited for highly stereotyped sound events. As it doesn't depend on the signal-to-noise ratio it's more robust to higher levels of background noise. The procedure is divided in three steps:\n\n-   Choosing the right template (`get_templates()`)\n-   Estimating the cross-correlation scores of templates along sound files (`template_correlator()`)\\\n-   Detecting sound events by applying a correlation threshold (`template_detector()`)\n\nThe function `get_templates()` can help you find a template closer to the average acoustic structure of the sound events in a reference table. This is done by finding the sound events closer to the centroid of the acoustic space. When the acoustic space is not supplied ('acoustic.space' argument) the function estimates it by measuring several acoustic parameters using the function [`spectro_analysis()`](https://marce10.github.io/warbleR/reference/spectro_analysis.html) from [`warbleR`](https://CRAN.R-project.org/package=warbleR)) and summarizing it with Principal Component Analysis (after z-transforming parameters). If only 1 template is required the function returns the sound event closest to the acoustic space centroid. The rationale here is that a sound event closest to the average sound event structure is more likely to share structural features with most sounds across the acoustic space than a sound event in the periphery of the space. These 'mean structure' templates can be obtained as follows:\n\n```{r, eval = FALSE, echo = TRUE}\n\n# get mean structure template\ntemplate <-\n  get_templates(reference = lbh1_reference, path = tempdir())\n\n```\n\n```{r, fig.width=6, fig.height=5, eval = TRUE, echo = FALSE}\n\npar(mar = c(5, 4, 1, 1), bg = \"white\")\n\n# get mean structure template\ntemplate <-\n  get_templates(reference = lbh1_reference, path = tempdir())\n\n```\n\n \n\nThe graph above shows the overall acoustic spaces, in which the sound closest to the space centroid is highlighted. The highlighted sound is selected as the template and can be used to detect similar sound events. The function `get_templates()` can also select several templates. This can be helpful when working with sounds that are just moderately stereotyped. This is done by dividing the acoustic space into sub-spaces defined as equal-size slices of a circle centered at the centroid of the acoustic space:\n\n```{r, eval = FALSE, echo = TRUE}\n\n# get 3 templates\nget_templates(reference = lbh_reference, \n                          n.sub.spaces = 3, path = tempdir())\n\n```\n\n```{r, fig.width=6, fig.height=5, eval = TRUE, echo = FALSE}\n\npar(mar = c(5, 4, 1, 1), bg = \"white\")\n\n# get 3 templates\ntemplates <- get_templates(reference = lbh_reference, \n                          n.sub.spaces = 3, path = tempdir())\n\n```\n\nWe will use the single template object ('template') to run a detection on the example 'lbh1' data:\n\n```{r}\n\n# get correlations\ncorrelations <-\n  template_correlator(templates = template,\n                      files = \"lbh1.wav\",\n                      path = tempdir())\n\n```\n\n \n\nThe output is an object of class 'template_correlations', with its own printing method:\n\n```{r}\n\n# print\ncorrelations\n\n```\n\n \n\nThis object can then be used to detect sound events using `template_detector()`:\n\n```{r}\n\n# run detection\ndetection <-\n  template_detector(template.correlations = correlations, threshold = 0.4)\n\ndetection\n```\n\n \n\nThe output can be explored by plotting the spectrogram along with the detection and correlation scores:\n\n```{r, warning=FALSE}\n\n# plot spectrogram\nlabel_spectro(\n  wave = lbh1,\n  detection = detection,\n  template.correlation = correlations[[1]],\n  flim = c(0, 10),\n  threshold = 0.4,\n  hop.size = 10, ovlp = 50)\n\n```\n\n \n\nThe performance can be evaluated using `diagnose_detection()`:\n\n```{r}\n\n#diagnose\ndiagnose_detection(reference = lbh1_reference, detection = detection)\n\n```\n\n \n\n### Optimizing template-based detection\n\nThe function `optimize_template_detector()` allows to evaluate the performance under different correlation thresholds:\n\n```{r}\n\n# run optimization\noptimization <-\n  optimize_template_detector(\n    template.correlations = correlations,\n    reference = lbh1_reference,\n    threshold = seq(0.1, 0.5, 0.1)\n  )\n\n# print output\noptimization\n```\n\n \n\nAdditional threshold values can be evaluated without having to run it all over again. We just need to supplied the output from the previous run with the argument `previous.output` (the same trick can be done when optimizing an energy-based detection):\n\n```{r}\n\n# run optimization\noptimize_template_detector(\n  template.correlations = correlations,\n  reference = lbh1_reference,\n  threshold = c(0.6, 0.7),\n  previous.output = optimization\n)\n\n```\n\n \n\nIn this case several threshold values can achieved an optimal detection.\n\n \n\n### Detecting several templates\n\nSeveral templates can be used within the same call. Here we correlate two templates on the two example sound files, taking one template from each sound file:\n\n```{r}\n\n# get correlations\ncorrelations <-\n  template_correlator(\n    templates = lbh_reference[c(1, 10),],\n    files = c(\"lbh1.wav\", \"lbh2.wav\"),\n    path = tempdir()\n  )\n\n# run detection\ndetection <-\n  template_detector(template.correlations = correlations, threshold = 0.5)\n\ncorrelations <-\n  template_correlator(\n    templates = lbh_reference[c(1, 10),],\n    files = c(\"lbh1.wav\", \"lbh2.wav\"),\n    path = tempdir()\n  )\n\n```\n\n \n\nNote that in these cases we can get the same sound event detected several times (duplicates), one by each template. We can check if that is the case just by diagnosing the detection:\n\n```{r}\n\n#diagnose\ndiagnose_detection(reference = lbh_reference, detection = detection)\n\n```\n\n \n\nDuplicates are shown as split positives. Fortunately, we can leave a single detected sound event by leaving only those with the highest correlation. To do this we first need to label each row in the detection using `label_detection()` and then remove duplicates using `filter_detection()`:\n\n```{r}\n\n# labeling detection\nlabeled <-\n  label_detection(reference = lbh_reference, detection = detection)\n\n```\n\nThis function adds a column ('detection.class') with the class label for each row:\n\n```{r}\n\ntable(labeled$detection.class)\n\n```\n\n \n\nNow we can filter out duplicates and diagnose the detection again, telling the function to select a single row per duplicate using the correlation score as a criterium (`by = \"scores\"`, this column is part of the `template_detector()` output):\n\n```{r}\n\n# filter\nfiltered <- filter_detection(detection = labeled, by = \"scores\")\n\n# diagnose\ndiagnose_detection(reference = lbh_reference, detection = filtered)\n```\n\n \n\nWe successfully get rid of duplicates and detected every single target sound event.\n\n------------------------------------------------------------------------\n\n## Improving detection speed\n\nDetection routines can take a long time when working with large amounts of acoustic data (e.g. large sound files and/or many sound files). These are some useful points to keep in mine when trying to make a routine more time-efficient:\n\n-   Always test procedures on small data subsets\n-   `template_detector()` is faster than `energy_detector()`\n-   Parallelization (see `parallel` argument in most functions) can significantly speed-up routines, but works better on Unix-based operating systems (linux and mac OS)\n-   Sampling rate matters: detecting sound events on low sampling rate files goes faster, so we should avoid having nyquist frequencies (sampling rate / 2) way higher than the highest frequency of the target sound events (sound files can be downsampled using warbleR's [`fix_sound_files()`](https://marce10.github.io/warbleR/reference/selection_table.html))\n-   Large sound files can make the routine crash, use `split_acoustic_data()` to split both reference tables and files into shorter clips.\n-   Think about using a computer with lots of RAM memory or a computer cluster for working on large amounts of data\n-   `thinning` argument (which reduces the size of the amplitude envelope) can also speed-up `energy_detector()`\n\n \n\n------------------------------------------------------------------------\n\n## Additional tips\n\n-   Use your knowledge about the sound event structure to determine the initial range for the tuning parameters in a detection optimization routine\n-   If people have a hard time figuring out where a target sound event occurs in a recording, detection algorithms will also have a hard time\n-   Several templates representing the range of variation in sound event structure can be used to detect semi-stereotyped sound events\n-   Make sure reference tables contain all target sound events and only the target sound events. The performance of the detection cannot be better than the reference itself.\n-   Avoid having overlapping sound events or several sound events as a single one (like a multi-syllable vocalization) in the reference table when running an energy-based detector\n-   Low-precision can be improved by training a classification model (e.g. random forest) to tell sound events from noise\n\n------------------------------------------------------------------------\n\n::: {.alert .alert-info}\nPlease cite [ohun](https://github.com/maRce10/ohun) like this:\n\nAraya-Salas, M. (2021), *ohun: diagnosing and optimizing automated sound event detection*. R package version 0.1.0.\n:::\n\n \n\n## References\n\n1.  Araya-Salas, M. (2021), ohun: diagnosing and optimizing automated sound event detection. R package version 0.1.0.\n2.  Araya-Salas M, Smith-Vidaurre G (2017) warbleR: An R package to streamline analysis of animal sound events. Methods Ecol Evol 8:184-191.\n3.  Khanna H., Gaunt S.L.L. & McCallum D.A. (1997). Digital spectrographic cross-correlation: tests of sensitivity. Bioacoustics 7(3): 209-234.\n4.  Knight, E.C., Hannah, K.C., Foley, G.J., Scott, C.D., Brigham, R.M. & Bayne, E. (2017). Recommendations for acoustic recognizer performance assessment with application to five common automated signal recognition programs. Avian Conservation and Ecology,\n5.  Macmillan, N. A., & Creelman, C.D. (2004). Detection theory: A user's guide. Psychology press.\n\n \n\n------------------------------------------------------------------------\n\n<font size=\"4\">Session information</font>\n\n```{r session info, echo=F}\n\nsessionInfo()\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"number-sections":true,"highlight-style":"pygments","css":["styles.css"],"output-file":"ohun.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","toc-location":"left","toc-title":"Contents","code-copy":true,"date":"today","title":"<font size=\"7\"><b><i>ohun</i>&#x3A; optimizing acoustic signal detection</b></font>"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}